{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./images/logo.png\" width=\"20%\"></img>\n",
    "</center>\n",
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk through the same pre-processing steps that we did with NLTK, but now using Spark NLP.  Before getting started, if you are running this notebook in Google Colab, the following 2 cells needs to be executed to run the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "LKVBPMdNcWh0",
    "outputId": "0e436feb-c5e1-4224-d31d-5a1137839ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~16.04-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
      "Collecting pyspark==2.4.4\n",
      "  Downloading pyspark-2.4.4.tar.gz (215.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7 MB 66 kB/s s eta 0:00:01               | 50.8 MB 11.9 MB/s eta 0:00:14     |██████████▎                     | 69.2 MB 14.6 MB/s eta 0:00:11     |█████████████▊                  | 92.2 MB 7.0 MB/s eta 0:00:18     |██████████████▍                 | 97.2 MB 5.9 MB/s eta 0:00:20     |█████████████████████▉          | 147.2 MB 3.9 MB/s eta 0:00:18     |██████████████████████          | 148.3 MB 3.9 MB/s eta 0:00:18     |██████████████████████▍         | 150.9 MB 17.7 MB/s eta 0:00:04     |███████████████████████         | 154.7 MB 17.7 MB/s eta 0:00:04     |███████████████████████         | 155.4 MB 17.7 MB/s eta 0:00:04     |███████████████████████▍        | 157.5 MB 3.5 MB/s eta 0:00:17     |████████████████████████▌       | 165.4 MB 12.1 MB/s eta 0:00:05     |██████████████████████████▌     | 178.6 MB 9.8 MB/s eta 0:00:04     |█████████████████████████████▎  | 197.3 MB 12.7 MB/s eta 0:00:02\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/anna/nlp-fun/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-u9ncp2re/pyspark/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-u9ncp2re/pyspark/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-install-u9ncp2re/pyspark/pip-egg-info\n",
      "         cwd: /tmp/pip-install-u9ncp2re/pyspark/\n",
      "    Complete output (37 lines):\n",
      "    Could not import pypandoc - required to package PySpark\n",
      "    /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'\n",
      "      warnings.warn(msg)\n",
      "    zip_safe flag not set; analyzing archive contents...\n",
      "    pypandoc.__pycache__.__init__.cpython-35: module references __file__\n",
      "    \n",
      "    Installed /tmp/pip-install-u9ncp2re/pyspark/.eggs/pypandoc-1.5-py3.5.egg\n",
      "    Searching for wheel>=0.25.0\n",
      "    Reading https://pypi.python.org/simple/wheel/\n",
      "    Best match: wheel 0.34.2\n",
      "    Downloading https://files.pythonhosted.org/packages/75/28/521c6dc7fef23a68368efefdcd682f5b3d1d58c2b90b06dc1d0b805b51ae/wheel-0.34.2.tar.gz#sha256=8788e9155fe14f54164c1b9eb0a319d98ef02c160725587ad60f14ddc57b6f96\n",
      "    Processing wheel-0.34.2.tar.gz\n",
      "    Writing /tmp/easy_install-wjlctdmj/wheel-0.34.2/setup.cfg\n",
      "    Running wheel-0.34.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-wjlctdmj/wheel-0.34.2/egg-dist-tmp-5j0f0_dw\n",
      "    warning: no files found matching '*.dynlib' under directory 'tests'\n",
      "    no previously-included directories found matching 'tests/testdata/*/build'\n",
      "    no previously-included directories found matching 'tests/testdata/*/dist'\n",
      "    no previously-included directories found matching 'tests/testdata/*/*.egg-info'\n",
      "    warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
      "    \n",
      "    zip_safe flag not set; analyzing archive contents...\n",
      "    Moving UNKNOWN-0.0.0-py3.5.egg to /tmp/pip-install-u9ncp2re/pyspark/.eggs\n",
      "    \n",
      "    Installed /tmp/pip-install-u9ncp2re/pyspark/.eggs/UNKNOWN-0.0.0-py3.5.egg\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-u9ncp2re/pyspark/setup.py\", line 224, in <module>\n",
      "        'Programming Language :: Python :: Implementation :: PyPy']\n",
      "      File \"/usr/lib/python3.5/distutils/core.py\", line 108, in setup\n",
      "        _setup_distribution = dist = klass(attrs)\n",
      "      File \"/home/anna/nlp-fun/lib/python3.5/site-packages/setuptools/dist.py\", line 269, in __init__\n",
      "        self.fetch_build_eggs(attrs['setup_requires'])\n",
      "      File \"/home/anna/nlp-fun/lib/python3.5/site-packages/setuptools/dist.py\", line 313, in fetch_build_eggs\n",
      "        replace_conflicting=True,\n",
      "      File \"/home/anna/nlp-fun/lib/python3.5/site-packages/pkg_resources/__init__.py\", line 829, in resolve\n",
      "        raise DistributionNotFound(req, requirers)\n",
      "    pkg_resources.DistributionNotFound: The 'wheel>=0.25.0' distribution was not found and is required by pypandoc\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/anna/nlp-fun/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25hCollecting spark-nlp==2.5.0\n",
      "  Downloading spark_nlp-2.5.0-py2.py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-2.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/anna/nlp-fun/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install the appropriate packages\n",
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give permission to colab to access files from your google drive.  This permission lasts for just this Colab session.\n",
    "#You will need to click on the link that pops up, give Colab permission, and copy and paste the string into the box\n",
    "#that pops up down below.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "9aE_a9r5cNSM",
    "outputId": "f2d815a0-bacb-4d75-ac37-f92166cc7be7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import sparknlp and confirm all is installed properly with the following commands\n",
    "\n",
    "import sparknlp\n",
    "# Start Spark Session with Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzLz3dicFqzj"
   },
   "source": [
    "## Steps to pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf0fteXYFqzj"
   },
   "source": [
    "Steps 1-3 are some typical steps taken to clean and process the data to prepare our features (step 4).\n",
    "\n",
    "1. Tokenize\n",
    "2. Perform stemming/lemmatization\n",
    "3. Remove stop words\n",
    "4. Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOO8E9cVFqzk"
   },
   "source": [
    "Today, we're going to be working with a text loaded in the following cell for all our pre-processing steps.  The python package, Pandas, is a convenient way to read in the data and use it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AO4pzbYOFqzk"
   },
   "outputs": [],
   "source": [
    "#this root is set assuming you are using Google Colab.  If you are not, you can set it to root = './data/'\n",
    "root = '/content/content/My Drive/Colab Notebooks/data/'\n",
    "\n",
    "data_path = f'{root}preprocess_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Y8SA303GZcV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELvlScJUFqzo"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, sep='\\n', header = None, names=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQYT9_TJFqzs"
   },
   "source": [
    "### Data Prep before pre-processing\n",
    "\n",
    "Before we can do any of our pre-prcessing steps with Spark NLP, we must put our data into the proper format.  This involves casting our pandas dataframe to a Spark dataframe, and then creating a document object out of each text in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-ve7QoNIpmo5",
    "outputId": "f9ba1a92-2d3c-4122-b2cb-bc55d71e4952"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XERMYs0lp55F"
   },
   "outputs": [],
   "source": [
    "#Cast our pandas dataframe into a spark dataframe\n",
    "spark_df = spark.createDataFrame(data.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "m0AtV8eLp8L2",
    "outputId": "ac2d50ba-e63d-4233-b45f-625b1a3ef85a"
   },
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xo4j7D9yeja5"
   },
   "outputs": [],
   "source": [
    "#Use the DocumentAssembler to create document objects our of texts in our dataset\n",
    "from sparknlp.base import DocumentAssembler\n",
    "\n",
    "#To do: \n",
    "#1. Instantiate DocumentAssembler \n",
    "\n",
    "#2. Specify input column\n",
    "\n",
    "#3. Specify output column\n",
    "\n",
    "#4. Transform the spark dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe that is ready to pre-process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ubCIR_66Hr8Z",
    "outputId": "ca31d6dd-cf6a-4408-8f4c-68ca600f3cdf"
   },
   "outputs": [],
   "source": [
    "doc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEAWiNiZFqz4"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28jFCQKgFqz5"
   },
   "source": [
    "**Tokenization**: Segmentation of text into words (a form of feature extraction)\n",
    "<div align=\"center\">\n",
    "  <img height = 400, width = 400, src=\"./images/tokenize4.jpg\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i59-mk-WFqz5"
   },
   "source": [
    "Spark NLP has a single function for tokenization that is very convenient to use.  It has several different parameters that can be set in order to change how you do tokenization, all listed within the one tokenization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNW9xT0hFqz5"
   },
   "source": [
    "First, we will simply tokenize our text by splitting up the sentence into a list of words, symbols and numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gZl1EmYFqz8"
   },
   "outputs": [],
   "source": [
    "#To do:  Instantiate tokenizer and set columns, tokenize documents.\n",
    "\n",
    "from sparknlp.annotator import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "#set columns\n",
    "\n",
    "#fit and transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "l367EIY4IDKt",
    "outputId": "45c7e46e-e93c-41c8-881e-b1d9d60c2b12"
   },
   "outputs": [],
   "source": [
    "token_df.select('token.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rl6x1aBfFq0B"
   },
   "source": [
    "Just as we saw last week with NLTK, and the RegexpTokenizer(), there is a way to tokenize by using regular expressions in the Spark NLP tokenizer function.  The following example will show how to exclude punctuation from our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKPqp2weFq0C"
   },
   "outputs": [],
   "source": [
    "#Use the same tokenizer instance, but now give it a target pattern to split on.\n",
    "tokenizer.setTargetPattern('\\w+')\n",
    "token_df=tokenizer.fit(doc_df)\n",
    "token_df = token_df.transform(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "so9fVJ_kI18X",
    "outputId": "9dc3d8c0-c5d7-4352-db3d-48230e00b63c"
   },
   "outputs": [],
   "source": [
    "token_df.select('token.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVkciiIzFq0G"
   },
   "source": [
    "### Remove Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MO5yK2kHFq0G"
   },
   "source": [
    "Removal of words that are not important from the information point of view, such as: the, is, a, etc.\n",
    "The Spark NLP library does not have a list of stopwords available as a starting point.  It is up to the user to provide a list of stop words.  I have saved off the list that is available in the NLTK library to use in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVYEkrN5Fq0H"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'{root}stopwords.txt', 'rb') as file:\n",
    "    stopwords = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeUhzWOYFq0I"
   },
   "outputs": [],
   "source": [
    "#To do: create StopWordsCleaner instance and remove stop words from tokens.\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fZJUeFA3J8DN",
    "outputId": "126accb7-2b0c-42f3-f194-79903693a188"
   },
   "outputs": [],
   "source": [
    "clean_token_df.select('cleanTokens.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtoEaq-JFq0K"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lh-LhwqvFq0K"
   },
   "source": [
    "**Stemming**: Reduces words to their root, but the root might not always result in an actual word.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img height = 300, width = 300, src=\"./images/stem2.jpg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UIcMDUrFq0L"
   },
   "source": [
    "\n",
    "Spark NLP has a single stemmer function.  You can assume that the stemmer is up to the latest standards in stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQeAWtEOFq0L"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Stemmer\n",
    "\n",
    "stemmer = Stemmer()\n",
    "stemmer.setInputCols([\"cleanTokens\"]) \n",
    "stemmer.setOutputCol(\"stem\")\n",
    "\n",
    "stem_df=stemmer.transform(clean_token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I1TpHgT3KM2G",
    "outputId": "0e400e60-b2d7-49e9-8bae-1634c5d24fbf"
   },
   "outputs": [],
   "source": [
    "stem_df.select('stem.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNqT9Au4Fq0N"
   },
   "source": [
    "Spark NLP also has a lemmatizer available if desired.  \n",
    "\n",
    "In addition, there is a function called 'normalizer' which does several things at once. Per the Spark NLP documentation a normalizer 'removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary.'  In other words, it can remove punctuation and reduce words to a root based off of a dictionary provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbmwDtgYFq0N"
   },
   "source": [
    "### Word Embedding: Representing Text as Numerical Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqeMMkSWFq0N"
   },
   "source": [
    "+ We first need to represent texts to numbers that the learning algorithm can process. \n",
    "+ To represent each word in the dataset, we will use the pre-trained `WordEmbeddingsModel` from Spark NLP library. \n",
    "\n",
    "Word embedding converts a text into a numerical vector.  There are several different methods to do this.  The WordEmbeddingsModel is a pre-trained model based on the [GloVe](https://nlp.stanford.edu/projects/glove/) algorithm.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7675DfrHFq0O"
   },
   "source": [
    "<div align=\"center\">\n",
    "      <img height = 350, width = 350, src=\"./images/one_hot2.jpg\">\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_zuqDgCTFq0O",
    "outputId": "7dd84053-72d6-4a5c-c0df-0b0807c1720c"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import WordEmbeddingsModel\n",
    "\n",
    "word_embeddings=WordEmbeddingsModel.pretrained()\n",
    "word_embeddings.setInputCols(['document','stem'])\n",
    "word_embeddings.setOutputCol('embeddings')\n",
    "\n",
    "embeddings_df=word_embeddings.transform(stem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "M5yWflGkOAUq",
    "outputId": "31ad071b-0a52-4f1d-c190-8ad9ad065183"
   },
   "outputs": [],
   "source": [
    "embeddings_df.select('embeddings.embeddings').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VePdHOlFq0Q"
   },
   "source": [
    "# Using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI2UU-J0Fq0Q"
   },
   "source": [
    "Now, let's process our actual dataset. With this, we can make this process even more streamlined by setting up a pipeline.  A pipeline is a set of actions set by the user in a specific order.  The pipeline will take in the dataframe and run through all the steps that you told it to do in an efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "ftDAgUPrFq0Q",
    "outputId": "53635414-9af2-463d-c116-0ba1d19ba45f"
   },
   "outputs": [],
   "source": [
    "#load in the covid19 tweet dataset. \n",
    "data_path = f\"{root}covid19_tweets.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df = df.rename(columns={'tweet': 'text'})\n",
    "\n",
    "#we need to cast this pandas dataframe as a spark dataframe.\n",
    "spark_df = spark.createDataFrame(df.astype(str))\n",
    "\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZI2i7S-sJBf"
   },
   "outputs": [],
   "source": [
    "#import the Pipeline function\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Every other instance needed for pre-processing have already been imported and set up.\n",
    "#documentAssembler\n",
    "#tokenizer\n",
    "#stop_words_cleaner\n",
    "#stemmer\n",
    "#word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-cqevfjsiNm"
   },
   "outputs": [],
   "source": [
    "#To do: set up the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxfSXyQ9sbHj"
   },
   "outputs": [],
   "source": [
    "#create the model specified by the pipeline, feed the Spark dataframe into it.\n",
    "pipelineModel = nlpPipeline.fit(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "joC8Gkkzs1Zb",
    "outputId": "f5b6ba9a-bd7f-4142-9598-0b8ad2ed4637"
   },
   "outputs": [],
   "source": [
    "#obtain results of the pipeline\n",
    "result = pipelineModel.transform(spark_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VL_iLtsotffa",
    "outputId": "d5c27153-fb03-47fa-9358-1955a3f49a64"
   },
   "source": [
    "Notice how fast this was to process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./images/logo.png\" width=\"25%\"></img>\n",
    "</center>\n",
    "Copyright Quansight LLC 2018-2020"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sparknlp_twitter_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
